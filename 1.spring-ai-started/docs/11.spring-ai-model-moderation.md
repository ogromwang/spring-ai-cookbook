# 内容审核

内容审核模型（Moderation Model）用于检测文本中的有害内容，如暴力、仇恨言论、色情内容等。

::: code-group

```java [java:基本用法]
@Autowired
private ModerationModel moderationModel;

// 审核内容
ModerationResponse response = moderationModel.call(
    new ModerationPrompt("这是一段需要审核的文本")
);

// 检查是否被标记
boolean flagged = response.getResult().isFlagged();

// 获取分类结果
Map<String, Boolean> categories = response.getResult().getCategories();
```

:::

## 参考文档

- [Spring AI Moderation Model 官方文档](https://docs.spring.io/spring-ai/reference/api/moderation.html)

## 继续深入

- [[11.spring-ai-model-moderation/index|内容审核]]

---
